
R version 4.4.2 (2024-10-31) -- "Pile of Leaves"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

- Project '~/research/weather-data-collector-spain' loaded. [renv 1.1.4]
> # get_historical_data.R
> # ----------------------
> # Purpose: Download and update historical daily weather data for Spain from the AEMET OpenData API.
> #
> # This script fetches historical daily climatological data using the 7 core variables
> # that are compatible across current observations, historical data, and forecast endpoints.
> #
> # Core Variables (Standardized):
> #   - ta: Air temperature (Â°C) - from tmed
> #   - tamax: Maximum temperature (Â°C) - from tmax 
> #   - tamin: Minimum temperature (Â°C) - from tmin
> #   - hr: Relative humidity (%) - from hrMedia
> #   - prec: Precipitation (mm) - from prec
> #   - vv: Wind speed (km/h) - from velmedia
> #   - pres: Atmospheric pressure (hPa) - from presMax
> #
> # Concurrency Control:
> #   - Set PREVENT_CONCURRENT_RUNS = TRUE to enable lockfile-based run prevention
> #   - Set PREVENT_CONCURRENT_RUNS = FALSE (default) to allow multiple concurrent runs
> #
> # Main Steps:
> #   1. Load dependencies and API key.
> #   2. Determine which dates are missing from the local dataset.
> #   3. Download missing data in chunks, handling API rate limits and errors.
> #   4. Append new data to the historical dataset.
> #
> # Usage:
> #   - Requires a valid API key in 'auth/keys.R' as 'my_api_key'.
> #   - Run as an R script. Output is written to 'data/output/daily_station_historical.csv.gz'.
> #
> # Dependencies: tidyverse, lubridate, data.table, curl, jsonlite
> #
> # Author: John Palmer
> # Date: 2025-08-22 (Updated for 7-variable standardization)
> 
> # Title ####
> # For downloading and preparing historical weather data. 
> 
> rm(list=ls())
> 
> ####Dependencies####
> library(tidyverse)
â”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€
âœ” dplyr     1.1.4     âœ” readr     2.1.5
âœ” forcats   1.0.0     âœ” stringr   1.5.1
âœ” ggplot2   3.5.2     âœ” tibble    3.3.0
âœ” lubridate 1.9.4     âœ” tidyr     1.3.1
âœ” purrr     1.1.0     
â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
âœ– dplyr::filter() masks stats::filter()
âœ– dplyr::lag()    masks stats::lag()
â„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> library(lubridate)
> library(data.table)

Attaching package: â€˜data.tableâ€™

The following objects are masked from â€˜package:lubridateâ€™:

    hour, isoweek, mday, minute, month, quarter, second, wday, week,
    yday, year

The following objects are masked from â€˜package:dplyrâ€™:

    between, first, last

The following object is masked from â€˜package:purrrâ€™:

    transpose

> library(curl)
Using libcurl 8.7.1 with OpenSSL/3.2.2

Attaching package: â€˜curlâ€™

The following object is masked from â€˜package:readrâ€™:

    parse_date

> library(jsonlite)

Attaching package: â€˜jsonliteâ€™

The following object is masked from â€˜package:purrrâ€™:

    flatten

> 
> # If you want to prevent concurrent runs of this script, set PREVENT_CONCURRENT_RUNS to TRUE.
> PREVENT_CONCURRENT_RUNS = FALSE
> 
> if(PREVENT_CONCURRENT_RUNS) {
+   # Prevent concurrent runs by creating a lockfile
+   # Lockfile management
+   lockfile <- "tmp/get_historical_data.lock"
+   # Check if lockfile exists
+   if (file.exists(lockfile)) {
+     cat("Another run is in progress. Exiting.\n")
+     quit(save = "no", status = 0)
+   }
+   # Create a temporary directory and lockfile
+   dir.create("tmp", showWarnings = FALSE)
+   file.create(lockfile)
+   # Ensure lockfile is removed on exit
+   on.exit(unlink(lockfile), add = TRUE)
+ }
> 
> # Load API keys
> source("auth/keys.R")
> 
> # Set locale to UTF-8 for proper encoding handling
> Sys.setlocale("LC_ALL", "en_US.UTF-8")
[1] ""
Warning message:
In Sys.setlocale("LC_ALL", "en_US.UTF-8") :
  OS reports request to set locale to "en_US.UTF-8" cannot be honored
> 
> # SETTING DATES ####
> # Set the start date for historical data collection
> start_date = as_date("2013-07-01")
> 
> # Set up curl handle with API key for authentication and increased timeout
> h <- new_handle()
> handle_setheaders(h, 'api_key' = my_api_key)
> handle_setopt(h, timeout = 60, connecttimeout = 30)  # Increase timeout values
> 
> # Generate sequence of all dates to check (from start_date to 4 days before today)
> all_dates = seq.Date(from = start_date, to=today()-4, by = "day")
> 
> # Load existing historical weather data
> if(file.exists("data/output/daily_station_historical.csv.gz")){
+ stored_weather_daily = fread("data/output/daily_station_historical.csv.gz")
+ } else{stored_weather_daily = NULL}
> 
> 
> # Reverse date order (latest first)
> all_dates = rev(all_dates)
> 
> # Identify which dates are missing from the local dataset
> if(!is.null(stored_weather_daily)){
+   these_dates = all_dates[which(!all_dates %in% unique(stored_weather_daily$date))]
+ } else{
+   these_dates = all_dates
+ }
> 
> # Set chunk size for API requests (reduced to avoid rate limits and timeouts)
> chunksize = 5
> 
> # Main download loop: only run if there are missing dates
> if(length(these_dates) > 0){
+ 
+ lapply(seq(1, length(these_dates), chunksize), function(j){
+   
+   this_chunk = these_dates[j:min(length(these_dates), (j+(chunksize-1)))]
+   
+   weather_daily = rbindlist(lapply(seq_along(this_chunk), function(i){
+     
+     start_date = this_chunk[i]
+     print(start_date)
+     
+     tryCatch(
+       expr = {
+         # Request historical daily climatological data for specific date
+         req = curl_fetch_memory(paste0('https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/', start_date, 'T00%3A00%3A00UTC/fechafin/', start_date, 'T23%3A59%3A59UTC/todasestaciones'), handle=h)
+         
+         wurl = fromJSON(rawToChar(req$content))$datos
+         
+         req = curl_fetch_memory(wurl)
+         this_string = rawToChar(req$content)
+         
+         # Set encoding to handle Spanish characters properly
+         Encoding(this_string) = "latin1"
+         
+         # Parse JSON and standardize variable names to match current observations
+         wdia  = fromJSON(this_string) %>% 
+           as_tibble() %>% 
+           select(
+             date = fecha, 
+             indicativo, 
+             ta = tmed,        # Mean temperature -> ta
+             tamax = tmax,     # Maximum temperature -> tamax  
+             tamin = tmin,     # Minimum temperature -> tamin
+             hr = hrMedia,     # Mean humidity -> hr
+             prec = prec,      # Precipitation -> prec
+             vv = velmedia,    # Wind speed -> vv
+             pres = presMax    # Pressure (using max) -> pres
+           ) %>% 
+           mutate(
+             date = as_date(date),
+             ta = as.numeric(str_replace(ta, ",", ".")),
+             tamax = as.numeric(str_replace(tamax, ",", ".")),
+             tamin = as.numeric(str_replace(tamin, ",", ".")),
+             hr = as.numeric(str_replace(hr, ",", ".")),
+             # Handle precipitation more carefully - it often contains "Ip" for trace amounts
+             prec = case_when(
+               is.na(prec) ~ NA_real_,
+               str_detect(prec, "Ip|ip") ~ 0.1,  # Trace precipitation = 0.1mm
+               prec == "" ~ NA_real_,
+               TRUE ~ suppressWarnings(as.numeric(str_replace(prec, ",", ".")))
+             ),
+             vv = as.numeric(str_replace(vv, ",", ".")),
+             pres = as.numeric(str_replace(pres, ",", "."))
+           ) %>% 
+           as.data.table()
+         return(wdia)
+         
+       },
+       error = function(e){ 
+         cat("ERROR on date", as.character(start_date), ":", e$message, "\n")
+         Sys.sleep(60)  # Longer sleep on error
+         return(NULL)
+       },
+       warning = function(w){
+         cat("WARNING on date", as.character(start_date), ":", w$message, "\n") 
+         return(NULL)
+       },
+       finally = {
+         # (Optional)
+         # Do this at the end before quitting the tryCatch structure...
+       }
+     )
+     
+   }))
+   
+   if(!is.null(stored_weather_daily)){
+   weather_daily = rbindlist(list(weather_daily, stored_weather_daily))
+   }
+   
+    print("writing chunk")
+    
+    fwrite(weather_daily, "data/output/daily_station_historical.csv.gz")
+    
+    print("pausing 60 seconds")
+    Sys.sleep(60)  # Increased pause between chunks
+    
+  })
+  
+ } else{
+   
+   print("Up to date - no historical data downloaded")
+ }
[1] "2025-08-18"
ERROR on date 2025-08-18 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-08-17"
ERROR on date 2025-08-17 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-08-16"
[1] "2025-08-15"
[1] "2025-08-14"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-08-13"
[1] "2025-08-12"
[1] "2025-08-11"
[1] "2025-08-10"
[1] "2025-08-09"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-08-08"
ERROR on date 2025-08-08 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-08-07"
[1] "2025-08-06"
[1] "2025-08-05"
[1] "2025-08-04"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-08-03"
ERROR on date 2025-08-03 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-08-02"
[1] "2025-08-01"
[1] "2025-07-31"
[1] "2025-07-30"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-07-29"
[1] "2025-07-28"
[1] "2025-07-27"
[1] "2025-07-26"
[1] "2025-07-25"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-07-24"
[1] "2025-07-23"
[1] "2025-07-22"
ERROR on date 2025-07-22 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-07-21"
ERROR on date 2025-07-21 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-07-20"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-07-19"
[1] "2025-07-18"
[1] "2025-07-17"
[1] "2025-07-16"
[1] "2025-07-15"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-07-14"
ERROR on date 2025-07-14 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-07-13"
[1] "2025-07-12"
[1] "2025-07-11"
[1] "2025-07-10"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-07-09"
[1] "2025-07-08"
[1] "2025-07-07"
[1] "2025-07-06"
[1] "2025-07-05"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-07-04"
[1] "2025-07-03"
[1] "2025-07-02"
[1] "2025-07-01"
[1] "2025-06-30"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-06-29"
[1] "2025-06-28"
[1] "2025-06-27"
[1] "2025-06-26"
[1] "2025-06-25"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-06-24"
[1] "2025-06-23"
[1] "2025-06-22"
[1] "2025-06-21"
[1] "2025-06-20"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-06-19"
ERROR on date 2025-06-19 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-06-18"
ERROR on date 2025-06-18 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-06-17"
[1] "2025-06-16"
[1] "2025-06-15"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-06-14"
ERROR on date 2025-06-14 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-06-13"
[1] "2025-06-12"
[1] "2025-06-11"
[1] "2025-06-10"
ERROR on date 2025-06-10 : lexical error: invalid bytes in UTF8 string.
                {   "descripcion" : "Límite de peticiones o caudal por
                     (right here) ------^
 
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-06-09"
[1] "2025-06-08"
[1] "2025-06-07"
[1] "2025-06-06"
[1] "2025-06-05"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-06-04"
[1] "2025-06-03"
[1] "2025-06-02"
[1] "2025-06-01"
[1] "2025-05-31"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-05-30"
[1] "2025-05-29"
[1] "2025-05-28"
[1] "2025-05-27"
[1] "2025-05-26"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-05-25"
ERROR on date 2025-05-25 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-05-24"
[1] "2025-05-23"
[1] "2025-05-22"
[1] "2025-05-21"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-05-20"
[1] "2025-05-19"
[1] "2025-05-18"
[1] "2025-05-17"
[1] "2025-05-16"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-05-15"
ERROR on date 2025-05-15 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-05-14"
[1] "2025-05-13"
[1] "2025-05-12"
[1] "2025-05-11"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-05-10"
[1] "2025-05-09"
ERROR on date 2025-05-09 : Failure when receiving data from the peer [opendata.aemet.es]:
OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 0 
[1] "2025-05-08"
ERROR on date 2025-05-08 : lexical error: invalid char in json text.
                                       <!DOCTYPE html><html><head><tit
                     (right here) ------^
 
[1] "2025-05-07"
[1] "2025-05-06"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-05-05"
[1] "2025-05-04"
[1] "2025-05-03"
[1] "2025-05-02"
[1] "2025-05-01"
[1] "writing chunk"
[1] "pausing 60 seconds"
[1] "2025-04-30"
[1] "2025-04-29"
[1] "2025-04-28"
[1] "2025-04-27"
[1] "2025-04-26"
[1] "writing chunk"
[1] "pausing 60 seconds"
